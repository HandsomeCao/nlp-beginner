{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T08:06:44.227Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pickle\n",
    "import pkuseg\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取\n",
    "1. 使用上周保存的已经分好训练集测试集的数据\n",
    "2. 并同样进行分词操作，对所有训练数据准备好一个词库\n",
    "3. 准备词库的单词和序号相互的索引，在其中加入`<UNK>`标记，代表未出现在词库中的词语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.299Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./datasets.pickle', 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.300Z"
    }
   },
   "outputs": [],
   "source": [
    "_STOP_WORDS = []\n",
    "with open('./stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    _STOP_WORDS = f.readlines()\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in _STOP_WORDS]\n",
    "\n",
    "def tokenize_words(line, filter_stopwords=True):\n",
    "    words = segmentor.cut(line)\n",
    "    if filter_stopwords:\n",
    "        words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = []\n",
    "segmentor = pkuseg.pkuseg()\n",
    "for data in datasets['train']:\n",
    "    words += tokenize_words(data[1])\n",
    "words = list(set(words))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.301Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx  = {word: i+1 for i, word in enumerate(words)}\n",
    "word2idx['<unk>'] = 0\n",
    "idx2word = {i+1: word for i, word in enumerate(words)}\n",
    "idx2word[0] = '<unk>'\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量\n",
    "1. 使用gensim读取词向量,(单词总数352217, 维度300)\n",
    "2. 从上面的词向量中缩小范围，只留下我们要用到的词语的词向量\n",
    "3. 定义编码函数，将句子变为对应的单词序号序列，有了序号才方便去词向量中寻找单词对应的向量,同时要解决句子长度不一致问题，每个句子分词之后长度是不同的，设`max_len=64`，超过就取前64，不足就补0.\n",
    "4. 将数据集转化为这种类型\n",
    "5. 下载词向量保存在当前文件夹下，我的词向量为`sgns.wiki.word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.303Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel = KeyedVectors.load_word2vec_format('sgns.wiki.word', binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.304Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = torch.zeros(len(words) + 1, 300)  # 因为<unk>，所以加1\n",
    "for word in wvmodel.index2word:\n",
    "    try:\n",
    "        index = word2idx[word]  # 若找到就保存这个词语的向量\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(word)) # 没找到词向量就为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.305Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_sample(tokenize_sample, max_len=64, pad=0):\n",
    "    features = []\n",
    "    for token in tokenize_sample:\n",
    "        if token in word2idx:\n",
    "            features.append(word2idx[token])\n",
    "        else:\n",
    "            features.append(0)\n",
    "    \n",
    "    if len(features) >= max_len:\n",
    "        return features[:max_len]\n",
    "    else:\n",
    "        while (len(features) < max_len):\n",
    "            features.append(pad)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.306Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = [encode_sample(tokenize_words(data[1])) for data in datasets['train']]\n",
    "train_labels = [int(data[0]) for data in datasets['train']]\n",
    "dev_features = [encode_sample(tokenize_words(data[1])) for data in datasets['dev']]\n",
    "dev_labels = [int(data[0]) for data in datasets['dev']]\n",
    "test_features = [encode_sample(tokenize_words(data[1])) for data in datasets['test']]\n",
    "test_labels = [int(data[0]) for data in datasets['test']]\n",
    "# dev_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义TextCNN\n",
    "对输入x,首先经过word_embedding，将其变成词向量对应的矩阵，再通过三个不同卷积核大小的卷积层得到不同的特征，将得到的特征拼接起来，最后通过一层全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.307Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_size, seq_len, num_labels, weight):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        # embedding固定不训练\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv1 = nn.Conv2d(1, 1, (3, embed_size))\n",
    "        self.conv2 = nn.Conv2d(1, 1, (4, embed_size))\n",
    "        self.conv3 = nn.Conv2d(1, 1, (5, embed_size))\n",
    "        self.pool1 = nn.MaxPool2d((seq_len - 3 + 1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((seq_len - 4 + 1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((seq_len - 5 + 1, 1))\n",
    "        self.linear = nn.Linear(3, num_labels)  # 只用3个卷积核，最后维\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # [64, 64, 300] (batch_size, seq_len, embed_size)=> [64, 1, 64, 300]\n",
    "        x = self.embedding(x).view(x.size(0), 1, x.size(1), -1)\n",
    "        x1 = self.pool1(F.relu(self.conv1(x)))  # 卷积后[64, 1, 62, 1] => 池化后 [64, 1, 1, 1]\n",
    "        x2 = self.pool2(F.relu(self.conv2(x)))  # [64, 1, 61, 1] => [64, 1, 1, 1]\n",
    "        x3 = self.pool3(F.relu(self.conv3(x)))\n",
    "        out = torch.cat((x1, x2, x3), -1)  #[64, 1, 1, 3]\n",
    "        out = out.view(x.size(0), 1, -1)  #[64, 1, 3]\n",
    "        out = self.linear(out).view(-1, self.num_labels)  #[64, 1, 2] => [64, 2]      \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义超参数，网络初始化，及数据预准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.308Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "embed_size = 300\n",
    "seq_len = 64\n",
    "num_labels = 2\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.309Z"
    }
   },
   "outputs": [],
   "source": [
    "net = TextCNN(embed_size, seq_len, num_labels, weight)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.310Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = TensorDataset(torch.tensor(train_features), torch.tensor(train_labels))\n",
    "val_set = TensorDataset(torch.tensor(dev_features), torch.tensor(dev_labels))\n",
    "test_set = TensorDataset(torch.tensor(test_features), torch.tensor(test_labels))\n",
    "\n",
    "train_iter = DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "val_iter = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_iter = DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = 0, 0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    m, n = 0, 0\n",
    "    iter_bar = tqdm(train_iter, desc='Iter Train')\n",
    "    for features, labels in iter_bar:\n",
    "        m += 1\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(features)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        train_acc += accuracy_score(torch.argmax(output.cpu().data, dim=1), labels.cpu())\n",
    "        train_loss += loss\n",
    "        \n",
    "    # val\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_iter:\n",
    "            n += 1\n",
    "            val_features = val_features.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            output = net(val_features)\n",
    "            loss = criterion(output, val_labels)\n",
    "            val_acc += accuracy_score(torch.argmax(output.cpu().data, dim=1), val_labels.cpu())\n",
    "            val_loss += loss\n",
    "    print('Epoch: {}, train_loss: {}, train_acc: {}, val_loss: {}, val_acc: {}'.format(\n",
    "            epoch, train_loss.data / m, train_acc / m, \n",
    "            val_loss.data / n, val_acc / n))\n",
    "### torch.save(net.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T08:09:33.194Z"
    }
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_acc = 0\n",
    "test_pre = 0\n",
    "for test_idx, (test_features, test_labels) in enumerate(test_iter):\n",
    "    test_features = test_features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = net(test_features) \n",
    "        out_index = torch.argmax(output.cpu().data,dim=1)\n",
    "        label = test_labels.cpu()\n",
    "        test_acc += accuracy_score(torch.argmax(output.cpu().data, dim=1), test_labels.cpu())\n",
    "        test_pre =(out_index == 1) & (label == 1).sum()\n",
    "print('Test acc is {}'.format(test_acc / (test_idx + 1)))\n",
    "print('Test pre is {}'.format(test_pre / (label == 1)))\n",
    "print('Test recall is {}'.format(test_pre / (out_index == 1)))\n",
    "F1 = 2*recall*pre/(recall+pre)\n",
    "print('Test f1 is {}'.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.314Z"
    }
   },
   "outputs": [],
   "source": [
    "out_index = torch.argmax(output.cpu().data,dim=1)\n",
    "out_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.315Z"
    }
   },
   "outputs": [],
   "source": [
    "label = test_labels.cpu()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.316Z"
    }
   },
   "outputs": [],
   "source": [
    "label.long() == out_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:54:41.317Z"
    }
   },
   "outputs": [],
   "source": [
    "x = (out_index == 1) & (label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
